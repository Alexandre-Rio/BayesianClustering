import numpy as np
import matplotlib.pyplot as plt

from utils import *
from stats import crp
from generate_data import GMM, toy_example
from computations import *

class BayesianClustering:
    def __init__(self, S, **kwargs):
        self.S = S
        self.d = kwargs['d']
        self.r0 = kwargs['r0']
        self.s0 = kwargs['s0']
        self.theta = kwargs['theta']
        self.ksi = kwargs['ksi']
        self.N_theta = self.theta.shape[0]
        self.K = kwargs['K_init']

        self.n = S.shape[0]
        self.c = None  # Cluster indices
        self.B_samples = []  # Membership matrices generated

    def posterior_theta(self, theta):
        ''' Compute the posteriors of the theta's. For computational stability, the log-likelihood is first computed '''
        n_js = np.unique(self.c, return_counts=True)[1]

        sum_term = 0
        log_prod_term = 0  # Product in the likelihood formula, which is a sum
        for j in range(1, self.K + 1):
            log_prod_term += np.log(1 + theta * n_js[j - 1])
            I_j = (self.c == j)
            S_jj = np.vdot(I_j, np.dot(self.S, I_j))
            sum_term += (theta * S_jj) / (1 + n_js[j - 1] * theta)
        log_prod_term *= - 0.5 * self.d

        exponent = - 0.5 * self.d * (self.n + self.r0)
        log_likelihood = log_prod_term + exponent * \
                         (np.log(0.5 * self.d) + np.log(np.trace(self.S) - sum_term + self.s0))

        return np.exp(log_likelihood) / self.N_theta

    def B_cond_ksi(self, c):
        ''' Compute the posterior of B conditional on ksi '''
        likelihood = self.ksi / (self.n - 1 + self.ksi)
        for i in range(1, self.n):
            c_curr = c[i]
            c_old = c[0: i]
            likelihood *= crp(c_curr, c_old, self.ksi)
        return likelihood

    def posterior_B(self, c, posterior_theta):
        ''' Compute the posterior of B conditional on S, theta, d and ksi '''
        B_cond_ksi = self.B_cond_ksi(c)
        return posterior_theta * B_cond_ksi / self.N_theta

    def mcmc_sweep(self):
        ''' One sweep of the MCMC algorithm '''
        # Step 1: Sample theta
        posteriors_theta = np.array([self.posterior_theta(self.theta[i]) for i in range(self.N_theta)])
        norm_posteriors_theta = posteriors_theta / posteriors_theta.sum()  # Normalize probabilities
        count_nan = np.count_nonzero(np.isnan(norm_posteriors_theta))  # Account for nan values
        if count_nan == 0:
            theta = np.random.choice(self.theta, p=norm_posteriors_theta)
        elif count_nan < len(self.theta):
            mask = np.isnan(norm_posteriors_theta)
            theta_temp = self.theta[~mask]
            norm_posteriors_theta = norm_posteriors_theta[~mask]
            norm_posteriors_theta /= norm_posteriors_theta.sum()
            theta = np.random.choice(theta_temp, p=norm_posteriors_theta)
        else:
            theta = np.random.choice(self.theta)

        index_theta = np.where(self.theta == theta)[0][0]
        if not np.isnan(posteriors_theta[index_theta]):
            posterior_theta = posteriors_theta[index_theta]  # Posterior value used to compute the posterior of B
        else:
            posterior_theta = 1

        # Step 2: Update membership vector
        for i in range(self.n):  # Iterate through observations
            pi = np.zeros(self.K + 1)
            for j in range(1, self.K + 2):  # Iterate though clusters + new cluster
                c_new = self.c.copy()
                c_new[i] = j
                pi[j - 1] = self.posterior_B(c_new, posterior_theta)
            norm_pi = pi / pi.sum()
            self.c[i] = np.random.choice(np.arange(1, self.K + 2), p=norm_pi)  # Update c_i
        self.K = len(np.unique(self.c))  # Update number of clusters

        # Step 3: Obtain new membership matrix
        B = membership_c2B(self.c)

        return B


    def mcmc_sampler(self, iter, burn_in=100):
        ''' MCMC posterior sampling algorithm; Generate a sequence of membership matrices'''
        # Initialization
        self.c = np.random.randint(1, self.K + 1, size=self.n)
        # self.c.sort()
        B = membership_c2B(self.c)
        self.B_samples.append(B)

        # Iterations
        for it in range(1, iter):
            print(it)
            B = self.mcmc_sweep()
            self.B_samples.append(B)

        if len(self.B_samples) > burn_in:
            B_samples = self.B_samples[burn_in:]
        else:
            B_samples = None

        return B_samples

    def extrinsic_mean(self, B_samples):
        ''' Compute extrinsic mean of the sequence of membership matrices generated by MCMC'''
        M = len(B_samples)

        # Step 1: Find the mode of clusters
        c_samples = [membership_B2c(B) for B in B_samples]
        nb_clusters = [len(np.unique(c_samples[i])) for i in range(len(c_samples))]
        count_nb_clusters = np.unique(nb_clusters, return_counts=True)
        mode = count_nb_clusters[0][count_nb_clusters[1].argmax()]

        # Step 2: Calculate euclidean mean and threshold it on the set of membership matrices
        B_mean = np.array(B_samples).mean(axis=0)
        k = self.n
        iter = M

        B_star_list = []
        while k != mode and iter >= 0:
            print(iter)
            J = np.arange(1, self.n + 1)
            B_star = np.zeros((self.n, self.n))
            iter -= 1
            t_star = iter / M

            for j in J:
                v = (B_mean[j - 1, :] > t_star)
                C = np.where(v == 1)[0]
                J = np.setdiff1d(J, C + 1)

                for i in C:
                    B_star[i, :], B_star[:, i] = v, v
                    B_mean[i, :], B_mean[:, i] = np.zeros(self.n), np.zeros(self.n)

            c = membership_B2c(B_star)
            k = len(np.unique(c))
            B_star_list.append(B_star)

        return B_star_list[-1]


if __name__ == '__main__':

    # Compute metric and estimate hyperpriors2
    S = toy_example() # Use toy example
    # S = np.load('data/S_tmp_40.npy')  # Use similarity matrix from Git
    d_eb = top95_eigenvalues(S)
    r = 3
    s = 4

    # Set parameters
    params = {'d': d_eb,
              'r0': 2 * r / d_eb,
              's0': 2 * s / d_eb,
              'theta': np.array([1000, 2000, 3000, 4000, 5000]),
              'ksi': 1,
              'K_init': 10
              }

    # Define and run Bayesian Clustering algorithm
    self = BayesianClustering(S, **params)
    B_samples = self.mcmc_sampler(4000, 1000)
    np.save('outputs/B_samples3.npy', np.array(B_samples))
    #B_samples = np.load('outputs/B_samples3.npy')
    #B_samples = B_samples[1000:]

    c_samples = [membership_B2c(B) for B in B_samples]
    nb_clusters = [len(np.unique(c_samples[i])) for i in range(len(c_samples))]
    count_nb_clusters = np.unique(nb_clusters, return_counts=True)
    mode = count_nb_clusters[0][count_nb_clusters[1].argmax()]

    # Show the distribution of the number of clusters
    plt.bar(count_nb_clusters[0], count_nb_clusters[1])
    plt.show()

    end = True